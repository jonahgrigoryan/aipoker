# Critical Testing Checkpoints for Poker Bot Development

Based on your implementation plan, here are the **mandatory stop-and-test gates** before proceeding. These checkpoints prevent cascading failures and ensure you're building on a solid foundation.

---

## ğŸ”´ CHECKPOINT 1: After Task 2 (Configuration Manager)
**Stop and Test Before Task 3**

### What to Validate
```bash
# Test suite to run
npm test -- config.test.ts
```

**Acceptance Criteria:**
- âœ… Load valid configuration without errors
- âœ… Hot-reload detects file changes within 1 second
- âœ… Invalid config triggers rollback (test with missing required fields)
- âœ… Schema validation catches type errors
- âœ… Subscription system notifies components on config changes

**Manual Verification:**
1. Start system with valid config â†’ should load
2. Edit config file with invalid JSON â†’ should rollback with clear error message
3. Change `strategy.alphaGTO` â†’ verify Strategy Engine receives update notification

**Why This Matters:** Config drives all components. If config fails, everything downstream breaks.

---

## ğŸ”´ CHECKPOINT 2: After Task 3 (Vision System + Parser) 
**Stop and Test Before Task 4** âš ï¸ **MOST CRITICAL CHECKPOINT**

### What to Validate

```bash
# Run golden test suite
npm test -- vision.golden.test.ts

# Run parser tests
npm test -- parser.test.ts

# Run SafeAction tests
npm test -- safeaction.test.ts
```

**Acceptance Criteria:**
- âœ… **Card recognition: â‰¥99.5% accuracy** on golden image set (10k+ images)
- âœ… **Latency: â‰¤50ms @ P95** for frame capture + extraction
- âœ… **Pot/stack error: â‰¤1 BB @ P95**
- âœ… **Position assignment: â‰¤0.1% error rate** over 1000 test hands
- âœ… **Confidence gating:** Frames with confidence <0.995 trigger SafeAction
- âœ… **Occlusion detection:** >5% occlusion triggers SafeAction
- âœ… **SafeAction policy:** Returns check-or-fold correctly for all game states

**Manual Verification:**
1. Run vision system on 5-10 different poker platform screenshots
2. Compare extracted values to ground truth
3. Occlude 10% of a card ROI â†’ should trigger SafeAction
4. Check latency distribution: `P50: __ms, P95: __ms, P99: __ms`

**Performance Benchmark:**
```typescript
// Run this benchmark
const results = await benchmarkVision(1000 frames);
console.log(`P50: ${results.p50}ms`);
console.log(`P95: ${results.p95}ms`); // MUST be â‰¤50ms
console.log(`Accuracy: ${results.accuracy}%`); // MUST be â‰¥99.5%
```

**Why This Matters:** Vision is the foundation. If you can't extract game state accurately, everything else is garbage-in-garbage-out. This is where 80% of bugs will originate.

---

## ğŸŸ¡ CHECKPOINT 3: After Task 4 (GTO Solver)
**Stop and Test Before Task 5**

### What to Validate

```bash
npm test -- gto.test.ts
npm run benchmark:gto
```

**Acceptance Criteria:**
- âœ… **Cache hit rate: â‰¥80%** for preflop situations
- âœ… **Subgame solve latency: â‰¤400ms** for uncached situations
- âœ… **Output validity:** Action frequencies sum to 1.0, EVs are reasonable
- âœ… **Deep-stack handling:** Different action abstractions applied when stack >100bb
- âœ… **Budget enforcement:** Returns cached policy when time budget would be exceeded

**Manual Verification:**
```typescript
// Test known equilibrium
const state = createTestState('preflop', 'BTN', 100bb);
const solution = await gtoSolver.solve(state, 400);

// Verify output structure
assert(solution.actions.size > 0);
assert(solution.computeTime <= 400);
assert(solution.source === 'cache' || solution.source === 'subgame');

// Check frequencies sum to 1.0
const totalFreq = Array.from(solution.actions.values())
  .reduce((sum, a) => sum + a.frequency, 0);
assert(Math.abs(totalFreq - 1.0) < 0.001);
```

**Performance Test:**
- Run 100 preflop solves â†’ measure cache hit rate
- Run 50 flop subgame solves â†’ measure P95 latency (should be â‰¤400ms)

**Why This Matters:** GTO solver provides the mathematical foundation for decisions. Bugs here lead to -EV play.

---

## ğŸŸ¡ CHECKPOINT 4: After Task 6 (Time Budget Tracker) + Task 5 (Agents)
**Stop and Test Before Task 8**

### What to Validate

```bash
npm test -- budget.test.ts
npm test -- agents.test.ts
npm test -- agent-schema.test.ts
```

**Acceptance Criteria - Time Budget:**
- âœ… **Budget tracking accurate** within 5ms
- âœ… **Preemption logic works:** Components honor shouldPreempt() checks
- âœ… **Dynamic adjustment:** Downstream budgets reduced when perception overruns

**Acceptance Criteria - Agents:**
- âœ… **Parallel execution:** All 3 agents queried simultaneously
- âœ… **Timeout enforcement:** Agents that don't respond in 3s are excluded
- âœ… **Schema validation:** Malformed JSON is discarded and counted toward timeout
- âœ… **Circuit breaker:** Trips after N consecutive failures, forces Î±=1.0
- âœ… **Cost tracking:** Token usage logged per hand

**Integration Test - End-to-End Timing:**
```typescript
// CRITICAL: Test if 2s deadline is achievable
const testCases = generateRandomGameStates(100);
const timings = [];

for (const state of testCases) {
  const start = performance.now();
  
  // Run full decision pipeline (mock execution)
  await visionSystem.capture();
  await parser.parse();
  await Promise.all([
    gtoSolver.solve(state, 400),
    agentCoordinator.queryAgents(state, 1200)
  ]);
  
  const elapsed = performance.now() - start;
  timings.push(elapsed);
}

const p95 = calculatePercentile(timings, 95);
console.log(`P95 latency: ${p95}ms`); // MUST be â‰¤2000ms
assert(p95 <= 2000, 'Failed to meet 2s deadline at P95');
```

**Why This Matters:** This tests if your latency budget is realistic. If you can't hit 2s @ P95 now, you won't hit it in production.

---

## ğŸŸ¢ CHECKPOINT 5: After Task 8 (Strategy Engine) + Task 7 (Risk Guard)
**Stop and Test Before Task 9**

### What to Validate

```bash
npm test -- strategy.test.ts
npm test -- risk-guard.test.ts
```

**Acceptance Criteria:**
- âœ… **Blending formula correct:** Î± Ã— GTO + (1-Î±) Ã— Exploit produces valid distributions
- âœ… **Action selection:** Samples correctly from distribution using seeded RNG
- âœ… **Bet sizing quantization:** Maps continuous to discrete set correctly
- âœ… **Divergence detection:** Logs when GTO/agent recommendations differ >30pp
- âœ… **Risk limits enforced:** Bankroll/session limits trigger panic stop
- âœ… **Fallback logic:** Returns SafeAction when risk exceeded or agents timeout

**Integration Test - Decision Quality:**
```typescript
// Test deterministic replay
const state = createTestState('flop', 'BTN', 50bb);
const seed = 12345;

const decision1 = await strategyEngine.decide(state, gto, agents, config, riskGuard, seed);
const decision2 = await strategyEngine.decide(state, gto, agents, config, riskGuard, seed);

// Should produce identical decisions
assert.deepEqual(decision1.action, decision2.action);
assert.equal(decision1.reasoning.alpha, decision2.reasoning.alpha);
```

**Why This Matters:** Strategy Engine is the brain. Bugs here mean you make wrong decisions even with perfect inputs.

---

## ğŸ”´ CHECKPOINT 6: After Task 9 (Action Executor)
**Stop and Test Before Task 13** âš ï¸ **CRITICAL FOR SAFETY**

### What to Validate

```bash
npm test -- executor.test.ts
npm test -- window-manager.test.ts
```

**Acceptance Criteria:**
- âœ… **Simulator mode works:** Direct API calls execute correctly
- âœ… **Compliance enforcement:** Research UI refuses execution on non-allowlisted sites
- âœ… **Window detection:** Finds poker windows with >99% accuracy
- âœ… **Button detection:** Locates action buttons with >99% accuracy
- âœ… **Coordinate conversion:** ROI â†’ screen coordinates accurate within 5px
- âœ… **Action verification:** Detects mismatches between expected/actual post-action state
- âœ… **Retry logic:** Re-evaluates once on mismatch, then halts

**Manual Verification (Research UI Mode):**
1. Start poker simulator in window
2. Run executor with valid allowlist â†’ should detect window and execute
3. Run executor with non-allowlisted site â†’ **should refuse and halt immediately**
4. Manually close poker window mid-execution â†’ should detect failure and halt

**Why This Matters:** This is where the bot interacts with the real world. Bugs here can cause compliance violations or lost money.

---

## ğŸ”´ CHECKPOINT 7: After Task 13 (Main Pipeline Integration)
**Stop and Test Before Task 15** âš ï¸ **END-TO-END VALIDATION**

### What to Validate

```bash
# Run full integration test suite
npm test -- integration.test.ts

# Run 100-hand smoke test
npm run smoke-test:100hands
```

**Acceptance Criteria:**
- âœ… **Full pipeline executes:** Vision â†’ Parser â†’ GTO + Agents â†’ Strategy â†’ Executor â†’ Logger
- âœ… **2s deadline met @ P95** across 100+ test hands
- âœ… **Compliance validation:** Startup checks refuse unauthorized environments
- âœ… **Error handling works:** Components fail gracefully, fallback policies triggered
- âœ… **Safe mode works:** Panic stop locks executor when triggered
- âœ… **Logging complete:** HandRecord contains all required fields (state, solver, agents, decision, timings, seeds, hashes)

**Smoke Test - 100 Hands:**
```bash
# Run against mock simulator
npm run smoke-test:100hands -- --config configs/test.json

# Verify output
- âœ… 100 hands completed without crashes
- âœ… P95 latency â‰¤ 2000ms
- âœ… All decisions logged with complete metadata
- âœ… No SafeAction triggers (vision confidence should be high in simulator)
- âœ… Win rate is non-negative (basic sanity check)
```

**Performance Validation:**
```typescript
// Check latency distribution for all components
const metrics = await sessionMetrics.get('smoke-test-100');

console.log('Vision P95:', metrics.perModuleLatency.get('vision').p95); // â‰¤50ms
console.log('GTO P95:', metrics.perModuleLatency.get('gto').p95);       // â‰¤400ms
console.log('Agents P95:', metrics.perModuleLatency.get('agents').p95); // â‰¤1200ms
console.log('Total P95:', metrics.latencyDistribution.p95);             // â‰¤2000ms
```

**Why This Matters:** This proves all components work together. If this passes, you have a working bot.

---

## ğŸŸ¢ CHECKPOINT 8: After Task 15 (Evaluation Framework)
**Final Validation Before Production**

### What to Validate

```bash
# Run offline evaluation smoke test (minimum)
npm run eval:offline-smoke -- --hands 10000

# Optional: Full 10M hand evaluation
npm run eval:offline-full -- --hands 10000000
```

**Acceptance Criteria (Smoke Test Minimum):**
- âœ… **Win rate â‰¥3bb/100** vs static opponent (tight-aggressive or loose-passive)
- âœ… **95% confidence interval** does not cross 0
- âœ… **No crashes** during extended run
- âœ… **Memory stable:** No leaks over 10k+ hands

**Acceptance Criteria (Full Evaluation - Optional):**
- âœ… **Win rate â‰¥3bb/100** vs static pool (10M hands)
- âœ… **Win rate â‰¥0bb/100** vs mixed GTO benchmark (10M hands)
- âœ… **Exploitability Îµ â‰¤ 0.02** vs baseline CFR bot

**Why This Matters:** This validates the bot actually plays well, not just that it doesn't crash.

---

## Testing Checkpoint Summary

| Checkpoint | After Task | Type | Time to Test | Failure Impact |
|-----------|-----------|------|--------------|----------------|
| 1ï¸âƒ£ Config | Task 2 | Unit | 10 min | ğŸŸ¡ Medium - blocks all components |
| 2ï¸âƒ£ Vision | Task 3 | Integration | **2-4 hours** | ğŸ”´ **CRITICAL** - foundation of system |
| 3ï¸âƒ£ GTO | Task 4 | Integration | 1 hour | ğŸŸ¡ Medium - decisions will be bad |
| 4ï¸âƒ£ Timing + Agents | Tasks 5-6 | Integration | 2 hours | ğŸŸ¡ Medium - won't meet deadline |
| 5ï¸âƒ£ Strategy | Tasks 7-8 | Integration | 1 hour | ğŸŸ¡ Medium - wrong decisions |
| 6ï¸âƒ£ Executor | Task 9 | Integration | **2 hours** | ğŸ”´ **CRITICAL** - compliance & safety |
| 7ï¸âƒ£ Full Pipeline | Task 13 | E2E | **3-4 hours** | ğŸ”´ **CRITICAL** - proves it works |
| 8ï¸âƒ£ Evaluation | Task 15 | Performance | 8+ hours | ğŸŸ¢ Low - validates quality |

---

## Recommended Testing Workflow

```mermaid
graph TD
    A[Task 2: Config] --> B{CP1: Config Tests Pass?}
    B -->|No| A
    B -->|Yes| C[Task 3: Vision + Parser]
    
    C --> D{CP2: Vision Golden Tests Pass?}
    D -->|No| C
    D -->|Yes| E[Tasks 4-6: GTO + Timing + Agents]
    
    E --> F{CP3-4: GTO + Agents Tests Pass?}
    F -->|No| E
    F -->|Yes| G[Tasks 7-8: Strategy + Risk]
    
    G --> H{CP5: Strategy Tests Pass?}
    H -->|No| G
    H -->|Yes| I[Task 9: Executor]
    
    I --> J{CP6: Executor Tests Pass?}
    J -->|No| I
    J -->|Yes| K[Task 13: Integration]
    
    K --> L{CP7: 100-Hand Smoke Test Pass?}
    L -->|No| K
    L -->|Yes| M[Task 15: Evaluation]
    
    M --> N{CP8: Offline Eval Pass?}
    N -->|No| M
    N -->|Yes| O[Production Ready]
```

---

## Key Takeaways

1. **Don't skip Checkpoint 2 (Vision)** - this is where 80% of bugs will come from
2. **Don't skip Checkpoint 7 (Full Pipeline)** - this proves everything works together
3. **Checkpoint 6 (Executor)** is critical for compliance - test thoroughly
4. **Budget 20-30 hours total** for all testing checkpoints
5. **Use CI to enforce gates** - fail build if checkpoints don't pass

---

## Quick Reference: Critical Tests Per Task

### Task 2 (Config Manager)
- [ ] `config.test.ts` - Load, validate, hot-reload
- [ ] Manual test: Edit config â†’ verify rollback on invalid

### Task 3 (Vision + Parser)
- [ ] `vision.golden.test.ts` - 10k image accuracy test (**â‰¥99.5%**)
- [ ] `parser.test.ts` - Position assignment, state-sync
- [ ] `safeaction.test.ts` - Check-or-fold policy
- [ ] Performance: P95 latency **â‰¤50ms**

### Task 4 (GTO Solver)
- [ ] `gto.test.ts` - Cache hits, subgame solve
- [ ] Benchmark: P95 **â‰¤400ms**

### Tasks 5-6 (Agents + Budget)
- [ ] `agents.test.ts` - Parallel execution, timeout
- [ ] `agent-schema.test.ts` - JSON validation
- [ ] `budget.test.ts` - Tracking, preemption
- [ ] Integration: End-to-end **â‰¤2000ms @ P95**

### Tasks 7-8 (Strategy + Risk)
- [ ] `strategy.test.ts` - Blending, bet sizing
- [ ] `risk-guard.test.ts` - Limits, panic stop
- [ ] Deterministic replay test

### Task 9 (Executor)
- [ ] `executor.test.ts` - Simulator/API/UI modes
- [ ] `window-manager.test.ts` - Detection, coords
- [ ] Compliance: Refuse non-allowlisted sites

### Task 13 (Integration)
- [ ] `integration.test.ts` - Full pipeline
- [ ] 100-hand smoke test

### Task 15 (Evaluation)
- [ ] Offline eval: **â‰¥3bb/100** vs static (10k+ hands)
- [ ] Memory leak check

---

## CI Pipeline Requirements

```yaml
# .github/workflows/test.yml
name: Poker Bot Testing Gates

on: [push, pull_request]

jobs:
  checkpoint-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkpoint 1 - Config
        run: npm test -- config.test.ts
        
      - name: Checkpoint 2 - Vision Golden
        run: |
          npm test -- vision.golden.test.ts
          npm test -- parser.test.ts
          npm test -- safeaction.test.ts
        
      - name: Vision Performance Check
        run: |
          npm run benchmark:vision
          # Fail if P95 > 50ms
        
      - name: Checkpoint 3 - GTO
        run: npm test -- gto.test.ts
        
      - name: Checkpoint 4 - Agents + Budget
        run: |
          npm test -- agents.test.ts
          npm test -- agent-schema.test.ts
          npm test -- budget.test.ts
        
      - name: Checkpoint 5 - Strategy + Risk
        run: |
          npm test -- strategy.test.ts
          npm test -- risk-guard.test.ts
        
      - name: Checkpoint 6 - Executor
        run: |
          npm test -- executor.test.ts
          npm test -- window-manager.test.ts
        
      - name: Checkpoint 7 - Integration
        run: |
          npm test -- integration.test.ts
          npm run smoke-test:100hands
        
      - name: Fail if P95 exceeds 2000ms
        run: npm run validate:latency
```

---

**Document Version:** 1.0  
**Last Updated:** Based on tasks.md and requirements.md  
**Status:** Ready for implementation

